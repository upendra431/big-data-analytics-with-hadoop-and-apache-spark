{"paragraphs":[{"text":"%md\n\n###02. Read Parquet Files into Spark\n\nRead a non-partitioned Parquet file into Spark. Measure the time taken. Also look at the execution plan.","user":"anonymous","dateUpdated":"2019-12-04T22:53:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>02. Read Parquet Files into Spark</h3>\n<p>Read a non-partitioned Parquet file into Spark. Measure the time taken. Also look at the execution plan.</p>\n"}]},"apps":[],"jobName":"paragraph_1575242453825_-267258794","id":"20191201-232053_1881388676","dateCreated":"2019-12-01T23:20:53+0000","dateStarted":"2019-12-04T22:53:37+0000","dateFinished":"2019-12-04T22:53:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4445"},{"text":"%spark2\n\n//Read the file\nval salesParquet = spark.read\n                        .parquet(\"/user/raj_ops/raw_parquet\")\n\n//Display the results and time the operation                     \nspark.time(salesParquet.show(5))\n\n//Show the execution Plan\nprintln(\"\\n-------------------------------EXPLAIN------------------------------------\")\nsalesParquet.explain\nprintln(\"-------------------------------END EXPLAIN--------------------------------\\n\")\n","user":"anonymous","dateUpdated":"2019-12-07T18:39:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+--------+----------+--------+-----+---------------+\n| ID|Customer| Product|      Date|Quantity| Rate|           Tags|\n+---+--------+--------+----------+--------+-----+---------------+\n|  1|   Apple|Keyboard|2019/11/21|       5|31.15|Discount:Urgent|\n|  2|LinkedIn| Headset|2019/11/25|       5| 36.9|  Urgent:Pickup|\n|  3|Facebook|Keyboard|2019/11/24|       5|49.89|           null|\n|  4|  Google|  Webcam|2019/11/07|       4|34.21|       Discount|\n|  5|LinkedIn|  Webcam|2019/11/21|       3|48.69|         Pickup|\n+---+--------+--------+----------+--------+-----+---------------+\nonly showing top 5 rows\n\nTime taken: 488 ms\n\n-------------------------------EXPLAIN------------------------------------\n== Physical Plan ==\n*(1) FileScan parquet [ID#147,Customer#148,Product#149,Date#150,Quantity#151,Rate#152,Tags#153] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/raw_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>\n-------------------------------END EXPLAIN--------------------------------\n\nsalesParquet: org.apache.spark.sql.DataFrame = [ID: int, Customer: string ... 5 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=8","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=9"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1575242483262_1337258092","id":"20191201-232123_1796936711","dateCreated":"2019-12-01T23:21:23+0000","dateStarted":"2019-12-07T18:39:50+0000","dateFinished":"2019-12-07T18:39:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4446"},{"text":"%md\n### 03.Read Partitioned Data into Spark\n\n\n","user":"anonymous","dateUpdated":"2019-12-01T23:34:12+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>03.Read Partitioned Data into Spark</h3>\n"}]},"apps":[],"jobName":"paragraph_1575242621638_1580960639","id":"20191201-232341_1286241066","dateCreated":"2019-12-01T23:23:41+0000","dateStarted":"2019-12-01T23:34:12+0000","dateFinished":"2019-12-01T23:34:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4447"},{"text":"%spark2\n\n//Read all the partitions. Use basePath to have partition key as part of data\nval salesPartitioned = spark.read\n                            .option(\"basePath\", \"/user/raj_ops/partitioned_parquet/\")\n                            .parquet(\"/user/raj_ops/partitioned_parquet/*\")\n                            \n//Display the results and time the operation                     \nspark.time(salesPartitioned.show(5))\n\n//Show the execution Plan\nprintln(\"\\n-------------------------------EXPLAIN------------------------------------\")\nsalesPartitioned.explain\nprintln(\"-------------------------------END EXPLAIN--------------------------------\\n\")\n\n","user":"anonymous","dateUpdated":"2019-12-07T18:46:46+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+----------+--------+-----+--------------------+-------+\n| ID|Customer|      Date|Quantity| Rate|                Tags|Product|\n+---+--------+----------+--------+-----+--------------------+-------+\n|  6|  Google|2019/11/23|       5|40.58|                null|  Mouse|\n|  8|  Google|2019/11/13|       1|46.79|Urgent:Discount:P...|  Mouse|\n| 14|   Apple|2019/11/09|       4|40.27|            Discount|  Mouse|\n| 15|   Apple|2019/11/25|       5|38.89|                null|  Mouse|\n| 20|LinkedIn|2019/11/25|       4|36.77|       Urgent:Pickup|  Mouse|\n+---+--------+----------+--------+-----+--------------------+-------+\nonly showing top 5 rows\n\nTime taken: 291 ms\n\n-------------------------------EXPLAIN------------------------------------\n== Physical Plan ==\n*(1) FileScan parquet [ID#185,Customer#186,Date#187,Quantity#188,Rate#189,Tags#190,Product#191] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionCount: 4, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n-------------------------------END EXPLAIN--------------------------------\n\nsalesPartitioned: org.apache.spark.sql.DataFrame = [ID: int, Customer: string ... 5 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=10","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=11"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1575242747048_1519812301","id":"20191201-232547_1252090573","dateCreated":"2019-12-01T23:25:47+0000","dateStarted":"2019-12-07T18:46:46+0000","dateFinished":"2019-12-07T18:46:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4448"},{"text":"%spark2\n\n//Read a specific partition only\nval salesHeadset = spark.read\n                            .parquet(\"/user/raj_ops/partitioned_parquet/Product=Headset\")\n\n//Display the results and time the operation                     \nspark.time(salesHeadset.show(5))\n\n//Show the execution Plan\nprintln(\"\\n-------------------------------EXPLAIN------------------------------------\")\nsalesHeadset.explain\nprintln(\"-------------------------------END EXPLAIN--------------------------------\\n\")\n","user":"anonymous","dateUpdated":"2019-12-07T18:52:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+----------+--------+-----+--------------------+\n| ID|Customer|      Date|Quantity| Rate|                Tags|\n+---+--------+----------+--------+-----+--------------------+\n|  2|LinkedIn|2019/11/25|       5| 36.9|       Urgent:Pickup|\n| 10|LinkedIn|2019/11/09|       2|26.91|Urgent:Discount:P...|\n| 11|Facebook|2019/11/26|       5|45.84|       Urgent:Pickup|\n| 12|  Google|2019/11/05|       2|41.17|     Discount:Urgent|\n| 17|   Apple|2019/11/09|       4|29.98|     Discount:Urgent|\n+---+--------+----------+--------+-----+--------------------+\nonly showing top 5 rows\n\nTime taken: 221 ms\n\n-------------------------------EXPLAIN------------------------------------\n== Physical Plan ==\n*(1) FileScan parquet [ID#289,Customer#290,Date#291,Quantity#292,Rate#293,Tags#294] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n-------------------------------END EXPLAIN--------------------------------\n\nsalesHeadset: org.apache.spark.sql.DataFrame = [ID: int, Customer: string ... 4 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=16","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=17"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1575242771143_779481656","id":"20191201-232611_2054349051","dateCreated":"2019-12-01T23:26:11+0000","dateStarted":"2019-12-07T18:52:50+0000","dateFinished":"2019-12-07T18:52:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4449"},{"text":"%md\n### 04. Read Bucketed Data into Spark\n","user":"anonymous","dateUpdated":"2019-12-01T23:34:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>04. Read Bucketed Data into Spark</h3>\n"}]},"apps":[],"jobName":"paragraph_1575243277820_-1249679787","id":"20191201-233437_677576354","dateCreated":"2019-12-01T23:34:37+0000","dateStarted":"2019-12-01T23:34:59+0000","dateFinished":"2019-12-01T23:34:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4450"},{"text":"%spark2\n\n//Read data from Hive\nval salesBucketed = sql(\"SELECT * FROM product_bucket_table\")\n\n//Display the results and time the operation                     \nspark.time(salesBucketed.show(5))\n\n//Show the execution Plan\nprintln(\"\\n-------------------------------EXPLAIN------------------------------------\")\nsalesBucketed.explain\nprintln(\"-------------------------------END EXPLAIN--------------------------------\\n\")\n","user":"anonymous","dateUpdated":"2019-12-07T18:54:36+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":225,"optionOpen":false}}},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+--------+----------+--------+-----+---------------+\n| ID|Customer| Product|      Date|Quantity| Rate|           Tags|\n+---+--------+--------+----------+--------+-----+---------------+\n|  1|   Apple|Keyboard|2019/11/21|       5|31.15|Discount:Urgent|\n|  3|Facebook|Keyboard|2019/11/24|       5|49.89|           null|\n|  4|  Google|  Webcam|2019/11/07|       4|34.21|       Discount|\n|  5|LinkedIn|  Webcam|2019/11/21|       3|48.69|         Pickup|\n|  7|LinkedIn|  Webcam|2019/11/20|       4|37.19|           null|\n+---+--------+--------+----------+--------+-----+---------------+\nonly showing top 5 rows\n\nTime taken: 219 ms\n\n-------------------------------EXPLAIN------------------------------------\n== Physical Plan ==\n*(1) FileScan parquet default.product_bucket_table[ID#110,Customer#111,Product#112,Date#113,Quantity#114,Rate#115,Tags#116] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/apps/spark/warehouse/product_bucket_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>\n-------------------------------END EXPLAIN--------------------------------\n\nsalesBucketed: org.apache.spark.sql.DataFrame = [ID: int, Customer: string ... 5 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=19"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1575243193524_71299787","id":"20191201-233313_1355861992","dateCreated":"2019-12-01T23:33:13+0000","dateStarted":"2019-12-07T18:54:36+0000","dateFinished":"2019-12-07T18:54:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4451"},{"text":"%spark2\n","user":"anonymous","dateUpdated":"2019-12-01T23:35:31+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575243331317_-1031655354","id":"20191201-233531_169085949","dateCreated":"2019-12-01T23:35:31+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:4452"}],"name":"SparkCourse/code_04_XX Data Extraction into Spark","id":"2ETBMHP8K","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}