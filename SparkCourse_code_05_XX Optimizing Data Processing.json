{"paragraphs":[{"text":"%md\n### 01. Pushing down Projections\n\nWhen downstream queries/processing only looks for a subset of columns, Spark optimizer is smart enough to identify them and only read those columns into the in-memory data frame. This saves on I/O and memory. This is called Projection Push down. While building data pipelines, it helps to be aware of how Spark works and take advantage of this for optimization.","user":"anonymous","dateUpdated":"2020-01-06T22:44:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>01. Pushing down Projections</h3>\n<p>When downstream queries/processing only looks for a subset of columns, Spark optimizer is smart enough to identify them and only read those columns into the in-memory data frame. This saves on I/O and memory. This is called Projection Push down. While building data pipelines, it helps to be aware of how Spark works and take advantage of this for optimization.</p>\n"}]},"apps":[],"jobName":"paragraph_1578350680813_-1272676219","id":"20191202-221915_958807702","dateCreated":"2020-01-06T22:44:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:368"},{"text":"%spark2\n\n//Read sales data from partitioned parquet file\nval salesData = spark.read\n                .option(\"basePath\", \"/user/raj_ops/partitioned_parquet/\")\n                .parquet(\"/user/raj_ops/partitioned_parquet/*\")\n                \n//Projection gets pushed down to the file scan\nprintln(\"-------------------------------EXPLAIN------------------------------------\")\nsalesData.select(\"Product\",\"Quantity\").explain\nprintln(\"-------------------------------END EXPLAIN--------------------------------\\n\")\n\n","user":"anonymous","dateUpdated":"2020-01-07T17:07:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"-------------------------------EXPLAIN------------------------------------\n== Physical Plan ==\n*(1) Project [Product#347, Quantity#344]\n+- *(1) FileScan parquet [Quantity#344,Product#347] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionCount: 4, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Quantity:int>\n-------------------------------END EXPLAIN--------------------------------\n\nsalesData: org.apache.spark.sql.DataFrame = [ID: int, Customer: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1578350680814_387746421","id":"20191202-221944_2145562378","dateCreated":"2020-01-06T22:44:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:369","dateFinished":"2020-01-07T17:07:39+0000","dateStarted":"2020-01-07T17:07:39+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=19"],"interpreterSettingId":"spark2"}}},{"text":"%md\n### 02. Pushing down Filters\n\nWhen downstream queries/processing only looks for a subset of subset, Spark optimizer is smart enough to identify them and only read those columns into the in-memory data frame. This saves on I/O and memory. This is called Filter Push down. This works for both partition columns and non-partition columns. While building data pipelines, it helps to be aware of how Spark works and take advantage of this for optimization.","user":"anonymous","dateUpdated":"2020-01-06T22:44:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>02. Pushing down Filters</h3>\n<p>When downstream queries/processing only looks for a subset of subset, Spark optimizer is smart enough to identify them and only read those columns into the in-memory data frame. This saves on I/O and memory. This is called Filter Push down. This works for both partition columns and non-partition columns. While building data pipelines, it helps to be aware of how Spark works and take advantage of this for optimization.</p>\n"}]},"apps":[],"jobName":"paragraph_1578350680814_-920399401","id":"20191202-222343_1548023469","dateCreated":"2020-01-06T22:44:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:370"},{"text":"%spark2\n\nval mouse = salesData.where($\"Product\" === \"Mouse\")\n\nprintln(\"-------------------------------EXPLAIN Filter by Partition------------------------------------\")\nmouse.explain\nprintln(\"-------------------------------END EXPLAIN--------------------------------\\n\")\n\n\nval google=salesData.where( $\"Customer\" === \"Google\")\n\nprintln(\"-------------------------------EXPLAIN Filter without Partition------------------------------------\")\ngoogle.explain\nprintln(\"-------------------------------END EXPLAIN--------------------------------\\n\")\n\nprintln();","user":"anonymous","dateUpdated":"2020-01-07T17:12:48+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"-------------------------------EXPLAIN Filter by Partition------------------------------------\n== Physical Plan ==\n*(1) FileScan parquet [ID#341,Customer#342,Date#343,Quantity#344,Rate#345,Tags#346,Product#347] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionCount: 1, PartitionFilters: [isnotnull(Product#347), (Product#347 = Mouse)], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n-------------------------------END EXPLAIN--------------------------------\n\n-------------------------------EXPLAIN Filter without Partition------------------------------------\n== Physical Plan ==\n*(1) Project [ID#341, Customer#342, Date#343, Quantity#344, Rate#345, Tags#346, Product#347]\n+- *(1) Filter (isnotnull(Customer#342) && (Customer#342 = Google))\n   +- *(1) FileScan parquet [ID#341,Customer#342,Date#343,Quantity#344,Rate#345,Tags#346,Product#347] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionCount: 4, PartitionFilters: [], PushedFilters: [IsNotNull(Customer), EqualTo(Customer,Google)], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n-------------------------------END EXPLAIN--------------------------------\n\n\nmouse: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ID: int, Customer: string ... 5 more fields]\ngoogle: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ID: int, Customer: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1578350680815_-1017797958","id":"20191202-222133_802924557","dateCreated":"2020-01-06T22:44:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:371","dateFinished":"2020-01-07T17:12:48+0000","dateStarted":"2020-01-07T17:12:48+0000"},{"text":"%md\n### 03.Partitioning and coalescing\n\nWhile performing actions, Spark creates results with the default partition count. In the case of Local mode, its usually equal to the number of cores. In the case of Clusters, the default is 200. This can be too much, if the number of cores in the cluster is significantly less than the number of partitions. So repartitioning helps to set the optimal number of partitions. \n\nRepartition does a full reshuffle and can be used for increasing/decreasing partitions.\n\nCoalasce simply consolidates existing partitions and avoids a full reshuffle. It can be used to decrease the number of partitions.\n\nRepartition and Coalasce themselves take significant time and resources. Do them only if multiple steps downstream will benefit from them.","user":"anonymous","dateUpdated":"2020-01-07T17:15:06+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>03.Partitioning and coalescing</h3>\n<p>While performing actions, Spark creates results with the default partition count. In the case of Local mode, its usually equal to the number of cores. In the case of Clusters, the default is 200. This can be too much, if the number of cores in the cluster is significantly less than the number of partitions. So repartitioning helps to set the optimal number of partitions.</p>\n<p>Repartition does a full reshuffle and can be used for increasing/decreasing partitions.</p>\n<p>Coalasce simply consolidates existing partitions and avoids a full reshuffle. It can be used to decrease the number of partitions.</p>\n<p>Repartition and Coalasce themselves take significant time and resources. Do them only if multiple steps downstream will benefit from them.</p>\n"}]},"apps":[],"jobName":"paragraph_1578350680815_-66521724","id":"20191202-222521_2048026507","dateCreated":"2020-01-06T22:44:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:372","dateFinished":"2020-01-07T17:15:08+0000","dateStarted":"2020-01-07T17:15:06+0000"},{"text":"%spark2\nprintln(\"Default parallelism : \" + sc.defaultParallelism + \"\\n\")\n//Optimal number of partitions = # of cores available.\n\nprintln(\"Partitions in SalesData from Parquet : \" + salesData.rdd.getNumPartitions + \"\\n\")\n\n//Read file without parallelizing\nval rawSalesData = spark\n                .read\n                .option(\"inferSchema\", \"true\")\n                .option(\"header\", \"true\")\n                .csv(\"/user/raj_ops/raw_data/sales_orders.csv\");\n                \nprintln(\"Partitions in raw CSV Read :\" + rawSalesData.rdd.getNumPartitions + \"\\n\")\n\n//Repartition to 8 partitions\nval partitionedSalesData = rawSalesData.repartition(8)\n\nprintln(\"Partitions after repartitioning :\" + partitionedSalesData.rdd.getNumPartitions + \"\\n\")\n\n//Coalesce to 3 partitions\nval coalasedSalesData = partitionedSalesData.coalesce(3)\n\nprintln(\"Partitions after coalese :\" + coalasedSalesData.rdd.getNumPartitions + \"\\n\")","user":"anonymous","dateUpdated":"2020-01-07T17:20:26+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Default parallelism : 2\n\nPartitions in SalesData from Parquet : 2\n\nPartitions in raw CSV Read :1\n\nPartitions after repartitioning :8\n\nPartitions after coalese :3\n\nrawSalesData: org.apache.spark.sql.DataFrame = [ID: int, Customer: string ... 5 more fields]\npartitionedSalesData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ID: int, Customer: string ... 5 more fields]\ncoalasedSalesData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ID: int, Customer: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1578350680816_-1830723731","id":"20191202-223029_518487198","dateCreated":"2020-01-06T22:44:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:373","dateFinished":"2020-01-07T17:20:27+0000","dateStarted":"2020-01-07T17:20:26+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=20","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=21"],"interpreterSettingId":"spark2"}}},{"text":"%md\n### 04.Managing Shuffling\n\nActions trigger shuffling. Shuffling takes time, memory and bandwidth. While building pipelines focus on \n\n- Minimize number of shuffles\n- Do actions late in the pipeline after data has been filtered.\n- Use aggregations by partition key as much as possible, as records with the same partition key stays in the same executor node.","user":"anonymous","dateUpdated":"2020-01-06T22:44:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>04.Managing Shuffling</h3>\n<p>Actions trigger shuffling. Shuffling takes time, memory and bandwidth. While building pipelines focus on</p>\n<ul>\n<li>Minimize number of shuffles</li>\n<li>Do actions late in the pipeline after data has been filtered.</li>\n<li>Use aggregations by partition key as much as possible, as records with the same partition key stays in the same executor node.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1578350680816_1554882575","id":"20191202-223104_470161728","dateCreated":"2020-01-06T22:44:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:374"},{"text":"%spark2\n\n\nval wordsRDD=spark.sparkContext\n                .parallelize(Seq(\"Google\", \"Apple\", \"Apple\", \"Google\", \n                    \"Google\", \"Apple\", \"Apple\", \"Apple\", \"Apple\", \"Apple\"))\n                    \n\n//Doing groupBy first. Shuffling has more data. Check DAG\n\nprint(\"For using group by key : \")\nval groupRDD = spark.time(\n                wordsRDD.map( word => (word, 1) )\n                    .groupByKey()\n                    .map( words => (words._1, words._2.sum ))\n                    .collect())\n\n//Doing reduce. Shuffling has less data. Check DAG\nprint(\"For using reduce : \")\nvar reduceRDD = spark.time(\n                wordsRDD.map( word => (word, 1) )\n                    .reduceByKey(_+_).collect())\n\n                \n//See content generated by groupByKey and reduceByKey\nprintln(\"\\nData shuffled after Group by Key : \")\nwordsRDD.map( word => (word, 1) )\n                    .groupByKey()\n                    .collect()\n                    .foreach(println)\n\nprintln(\"\\nData shuffled after Reduce by Key: \")                    \nwordsRDD.map( word => (word, 1) )\n                    .reduceByKey(_+_)\n                    .collect()\n                    .foreach(println)\n","user":"anonymous","dateUpdated":"2020-01-07T17:28:35+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"For using group by key : Time taken: 540 ms\nFor using reduce : Time taken: 361 ms\n\nData shuffled after Group by Key : \n(Apple,CompactBuffer(1, 1, 1, 1, 1, 1, 1))\n(Google,CompactBuffer(1, 1, 1))\n\nData shuffled after Reduce by Key: \n(Apple,7)\n(Google,3)\nwordsRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[100] at parallelize at <console>:24\ngroupRDD: Array[(String, Int)] = Array((Apple,7), (Google,3))\nreduceRDD: Array[(String, Int)] = Array((Apple,7), (Google,3))\n"}]},"apps":[],"jobName":"paragraph_1578350680816_334458387","id":"20191202-223950_1214415515","dateCreated":"2020-01-06T22:44:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:375","dateFinished":"2020-01-07T17:28:37+0000","dateStarted":"2020-01-07T17:28:35+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=22","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=23","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=24","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=25"],"interpreterSettingId":"spark2"}}},{"text":"%md\n### 05. Optimizing Joins\n\nBy default, joining two data frames require a lot of shuffling. If one data frame is considerably small, a better option is to broadcast that data frame to all the executors and then use those copies to join locally. Spark Optimizer chooses Broadcast joins when possible. Data frames within spark.sql.autoBroadcastJoinThreshold are automatically broadcasted","user":"anonymous","dateUpdated":"2020-01-06T22:44:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>05. Optimizing Joins</h3>\n<p>By default, joining two data frames require a lot of shuffling. If one data frame is considerably small, a better option is to broadcast that data frame to all the executors and then use those copies to join locally. Spark Optimizer chooses Broadcast joins when possible. Data frames within spark.sql.autoBroadcastJoinThreshold are automatically broadcasted</p>\n"}]},"apps":[],"jobName":"paragraph_1578350680817_-286588487","id":"20191202-224212_2090204976","dateCreated":"2020-01-06T22:44:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:376"},{"text":"%spark2\n\nval products = spark\n                .read\n                .option(\"inferSchema\", \"true\")\n                .option(\"header\", \"true\")\n                .csv(\"/user/raj_ops/raw_data/product_vendor.csv\");\n                \nproducts.show()\n\nimport org.apache.spark.sql.functions.broadcast\nprintln(\"-------------------------------EXPLAIN------------------------------------\")\nsalesData.join(broadcast(products),\"Product\").explain\nprintln(\"-------------------------------END EXPLAIN--------------------------------\\n\")\n\n\n","user":"anonymous","dateUpdated":"2020-01-07T17:38:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+-----------+\n| Product|     Vendor|\n+--------+-----------+\n|   Mouse|   Logitech|\n|Keyboard|  Microsoft|\n|  Webcam|   Logitech|\n| Headset|Plantronics|\n+--------+-----------+\n\n-------------------------------EXPLAIN------------------------------------\n== Physical Plan ==\n*(2) Project [Product#347, ID#341, Customer#342, Date#343, Quantity#344, Rate#345, Tags#346, Vendor#424]\n+- *(2) BroadcastHashJoin [Product#347], [Product#423], Inner, BuildRight\n   :- *(2) FileScan parquet [ID#341,Customer#342,Date#343,Quantity#344,Rate#345,Tags#346,Product#347] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionCount: 4, PartitionFilters: [isnotnull(Product#347)], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n      +- *(1) Project [Product#423, Vendor#424]\n         +- *(1) Filter isnotnull(Product#423)\n            +- *(1) FileScan csv [Product#423,Vendor#424] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/raw_data/product_vendor.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Product)], ReadSchema: struct<Product:string,Vendor:string>\n-------------------------------END EXPLAIN--------------------------------\n\nproducts: org.apache.spark.sql.DataFrame = [Product: string, Vendor: string]\nimport org.apache.spark.sql.functions.broadcast\n"}]},"apps":[],"jobName":"paragraph_1578350680817_-458297967","id":"20191202-224728_1681907436","dateCreated":"2020-01-06T22:44:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:377","dateFinished":"2020-01-07T17:38:28+0000","dateStarted":"2020-01-07T17:38:27+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=26","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=27","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=28"],"interpreterSettingId":"spark2"}}},{"text":"%md\n### 06.Storing Intermediate Results\n\nBy default, every time an action is performed, Spark executes all the previous steps right from the data read. This can end up being very expensive, especially while using Spark in a development or interactive mode. A better option is to cache intermediate results. Spark can cache in memory. It can also persist in both memory and disk. While running under YARN, persistance happens in HDFS by default.","user":"anonymous","dateUpdated":"2020-01-06T22:44:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>06.Storing Intermediate Results</h3>\n<p>By default, every time an action is performed, Spark executes all the previous steps right from the data read. This can end up being very expensive, especially while using Spark in a development or interactive mode. A better option is to cache intermediate results. Spark can cache in memory. It can also persist in both memory and disk. While running under YARN, persistance happens in HDFS by default.</p>\n"}]},"apps":[],"jobName":"paragraph_1578350680817_1783003612","id":"20191202-224752_434249283","dateCreated":"2020-01-06T22:44:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:378"},{"text":"%spark2\n\n//In memory only\nwordsRDD.cache()\n//Trigger an action for caching\nwordsRDD.collect()\n\nprintln(\"\\nPlan before caching intermediate results :\")\nval dataBefore = coalasedSalesData.where($\"Product\" === \"Mouse\")\ndataBefore.explain\n\n//Store on disk\nimport org.apache.spark.storage.StorageLevel\ncoalasedSalesData.persist(StorageLevel.DISK_ONLY)\n\n//Trigger an action for persisting.\ncoalasedSalesData.count()\n\nprintln(\"\\nPlan after caching :\")\nval dataAfter = coalasedSalesData.where($\"Product\" === \"Mouse\")\ndataAfter.explain","user":"anonymous","dateUpdated":"2020-01-06T22:44:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nPlan before caching intermediate results :\n== Physical Plan ==\nCoalesce 3\n+- Exchange RoundRobinPartitioning(8)\n   +- *(1) Project [ID#28, Customer#29, Product#30, Date#31, Quantity#32, Rate#33, Tags#34]\n      +- *(1) Filter (isnotnull(Product#30) && (Product#30 = Mouse))\n         +- *(1) FileScan csv [ID#28,Customer#29,Product#30,Date#31,Quantity#32,Rate#33,Tags#34] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/raw_data/sales_orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Product), EqualTo(Product,Mouse)], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>\n\nPlan after caching :\n== Physical Plan ==\n*(1) Filter (isnotnull(Product#30) && (Product#30 = Mouse))\n+- InMemoryTableScan [ID#28, Customer#29, Product#30, Date#31, Quantity#32, Rate#33, Tags#34], [isnotnull(Product#30), (Product#30 = Mouse)]\n      +- InMemoryRelation [ID#28, Customer#29, Product#30, Date#31, Quantity#32, Rate#33, Tags#34], true, 10000, StorageLevel(disk, 1 replicas)\n            +- Coalesce 3\n               +- Exchange RoundRobinPartitioning(8)\n                  +- *(1) FileScan csv [ID#28,Customer#29,Product#30,Date#31,Quantity#32,Rate#33,Tags#34] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/raw_data/sales_orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>\ndataBefore: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ID: int, Customer: string ... 5 more fields]\nimport org.apache.spark.storage.StorageLevel\ndataAfter: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ID: int, Customer: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1578350680818_441310840","id":"20191202-225505_1285607317","dateCreated":"2020-01-06T22:44:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:379"},{"text":"%spark2\n","user":"anonymous","dateUpdated":"2020-01-06T22:44:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578350680818_1196549567","id":"20191207-212508_115141898","dateCreated":"2020-01-06T22:44:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:380"}],"name":"SparkCourse/code_05_XX Optimizing Data Processing","id":"2EXR34ZND","noteParams":{},"noteForms":{},"angularObjects":{"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}