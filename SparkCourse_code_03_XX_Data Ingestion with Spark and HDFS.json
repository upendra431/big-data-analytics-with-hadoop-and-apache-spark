{"paragraphs":[{"text":"%md\n### 01. Reading Files into Spark \n\nData can be read into Apache Spark data frames from a variety of data sources. \n\nexamples : \n- A flat file on a local disk\n- A file from HDFS\n- A Kafka Topic\n\n\nIn this example, we will read a CSV file in a HDFS folder into a Spark Data Frame.","user":"anonymous","dateUpdated":"2019-12-04T21:11:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>01. Reading Files into Spark</h3>\n<p>Data can be read into Apache Spark data frames from a variety of data sources.</p>\n<p>examples :</p>\n<ul>\n<li>A flat file on a local disk</li>\n<li>A file from HDFS</li>\n<li>A Kafka Topic</li>\n</ul>\n<p>In this example, we will read a CSV file in a HDFS folder into a Spark Data Frame.</p>\n"}]},"apps":[],"jobName":"paragraph_1575242035908_-1377171045","id":"20191201-192816_210944076","dateCreated":"2019-12-01T23:13:55+0000","dateStarted":"2019-12-04T21:11:05+0000","dateFinished":"2019-12-04T21:11:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3269"},{"text":"%spark2\n\n//Check if everything is installed and working correctly\nprintln(\"Current installed version of Spark is \" + spark.version)\n\n","user":"anonymous","dateUpdated":"2019-12-07T17:45:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Current installed version of Spark is 2.3.1.3.0.1.0-187\n"}]},"apps":[],"jobName":"paragraph_1575242035908_637581075","id":"20191201-223537_1807095552","dateCreated":"2019-12-01T23:13:55+0000","dateStarted":"2019-12-07T17:45:57+0000","dateFinished":"2019-12-07T17:46:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3270"},{"text":"%spark2\n\n/*Read the raw CSV file int a Spark DataFrame\n    Use inferSchema to infer the schema automatically from the CSV file\n*/\nval rawSalesData = spark\n                .read\n                .option(\"inferSchema\", \"true\")\n                .option(\"header\", \"true\")\n                .csv(\"/user/raj_ops/raw_data/sales_orders.csv\");\n\n//Print the schema for verification\nrawSalesData.printSchema();\n\n//Print the first 5 records for verification\nrawSalesData.show(5)","user":"anonymous","dateUpdated":"2019-12-07T17:49:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- ID: integer (nullable = true)\n |-- Customer: string (nullable = true)\n |-- Product: string (nullable = true)\n |-- Date: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Rate: double (nullable = true)\n |-- Tags: string (nullable = true)\n\n+---+--------+--------+----------+--------+-----+---------------+\n| ID|Customer| Product|      Date|Quantity| Rate|           Tags|\n+---+--------+--------+----------+--------+-----+---------------+\n|  1|   Apple|Keyboard|2019/11/21|       5|31.15|Discount:Urgent|\n|  2|LinkedIn| Headset|2019/11/25|       5| 36.9|  Urgent:Pickup|\n|  3|Facebook|Keyboard|2019/11/24|       5|49.89|           null|\n|  4|  Google|  Webcam|2019/11/07|       4|34.21|       Discount|\n|  5|LinkedIn|  Webcam|2019/11/21|       3|48.69|         Pickup|\n+---+--------+--------+----------+--------+-----+---------------+\nonly showing top 5 rows\n\nrawSalesData: org.apache.spark.sql.DataFrame = [ID: int, Customer: string ... 5 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=0","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=1","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=2"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1575242035908_-2105395500","id":"20191201-224342_1438645233","dateCreated":"2019-12-01T23:13:55+0000","dateStarted":"2019-12-07T17:49:43+0000","dateFinished":"2019-12-07T17:49:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3271"},{"text":"%md\n### 02.Writing to HDFS\n\nWrite the rawSalesData Data Frame into HDFS as a Parquet file. Use Parquet as the format since it enables splitting and filtering. Use GZIP as the compression codec. \n\nOn completion, verify if the files are correctly through HDFS command line or Ambari","user":"anonymous","dateUpdated":"2019-12-04T21:19:46+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>02.Writing to HDFS</h3>\n<p>Write the rawSalesData Data Frame into HDFS as a Parquet file. Use Parquet as the format since it enables splitting and filtering. Use GZIP as the compression codec.</p>\n<p>On completion, verify if the files are correctly through HDFS command line or Ambari</p>\n"}]},"apps":[],"jobName":"paragraph_1575242035909_-1898016652","id":"20191201-224458_511802934","dateCreated":"2019-12-01T23:13:55+0000","dateStarted":"2019-12-04T21:19:46+0000","dateFinished":"2019-12-04T21:19:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3272"},{"text":"%spark2\n\n//Write to Sales Data to HDFS for future processing\n\nrawSalesData.write\n            .format(\"parquet\")\n            .mode(\"overwrite\")\n            .option(\"compression\", \"gzip\")\n            .save(\"/user/raj_ops/raw_parquet\");\n            ","user":"anonymous","dateUpdated":"2019-12-07T17:59:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=4"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1575242035909_1634333513","id":"20191201-225253_1705062044","dateCreated":"2019-12-01T23:13:55+0000","dateStarted":"2019-12-07T17:58:54+0000","dateFinished":"2019-12-07T17:58:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3273"},{"text":"%md\n### 03. Writing to HDFS with Partitioning\n\nWrite a partitioned Parquet file in HDFS. Partition will be done by Product. This will create one directory per unique product available in the raw CSV. Verify through HDFS command line or Ambari","user":"anonymous","dateUpdated":"2019-12-04T21:29:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>03. Writing to HDFS with Partitioning</h3>\n<p>Write a partitioned Parquet file in HDFS. Partition will be done by Product. This will create one directory per unique product available in the raw CSV. Verify through HDFS command line or Ambari</p>\n"}]},"apps":[],"jobName":"paragraph_1575242035909_-1312398621","id":"20191201-225634_1078208244","dateCreated":"2019-12-01T23:13:55+0000","dateStarted":"2019-12-04T21:29:26+0000","dateFinished":"2019-12-04T21:29:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3274"},{"text":"%spark2\n\nrawSalesData.write\n            .format(\"parquet\")\n            .mode(\"overwrite\")\n            .option(\"compression\", \"gzip\")\n            .partitionBy(\"Product\")\n            .save(\"/user/raj_ops/partitioned_parquet\")","user":"anonymous","dateUpdated":"2019-12-07T17:59:36+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1575242035909_1123488204","id":"20191201-230236_552761476","dateCreated":"2019-12-01T23:13:55+0000","dateStarted":"2019-12-03T03:07:30+0000","dateFinished":"2019-12-03T03:07:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3275"},{"text":"%md\n### 04. Writing to Hive with Bucketing\n\nCreate a Bucketed Hive table for orders. Bucketing will be done by Product. It will create 3 buckets based on the hash generated by Product. Hive tables can be queried through SQL.","user":"anonymous","dateUpdated":"2019-12-05T23:24:17+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>04. Writing to Hive with Bucketing</h3>\n<p>Create a Bucketed Hive table for orders. Bucketing will be done by Product. It will create 3 buckets based on the hash generated by Product. Hive tables can be queried through SQL.</p>\n"}]},"apps":[],"jobName":"paragraph_1575242035909_23755513","id":"20191201-230318_1214215207","dateCreated":"2019-12-01T23:13:55+0000","dateStarted":"2019-12-05T23:24:17+0000","dateFinished":"2019-12-05T23:24:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3276"},{"text":"%spark2\n\n//Create a Hive Table for sales data with 2 buckets.\nrawSalesData.write\n            .format(\"parquet\")\n            .mode(\"overwrite\")\n            .bucketBy(3, \"Product\")\n            .saveAsTable(\"product_bucket_table\")\n            \n//Data goes in here.\nprintln(\"Hive Data Stored in : \" + sc.getConf.get(\"spark.sql.warehouse.dir\") + \"\\n\")\n            \n//Read through SQL\nsql(\"SELECT * FROM product_bucket_table where Product='Mouse'\").show(5)\n            \n","user":"anonymous","dateUpdated":"2019-12-07T18:09:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Hive Data Stored in : /apps/spark/warehouse\n\n+---+--------+-------+----------+--------+-----+--------------------+\n| ID|Customer|Product|      Date|Quantity| Rate|                Tags|\n+---+--------+-------+----------+--------+-----+--------------------+\n|  6|  Google|  Mouse|2019/11/23|       5|40.58|                null|\n|  8|  Google|  Mouse|2019/11/13|       1|46.79|Urgent:Discount:P...|\n| 14|   Apple|  Mouse|2019/11/09|       4|40.27|            Discount|\n| 15|   Apple|  Mouse|2019/11/25|       5|38.89|                null|\n| 20|LinkedIn|  Mouse|2019/11/25|       4|36.77|       Urgent:Pickup|\n+---+--------+-------+----------+--------+-----+--------------------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=5","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=6","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=7"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1575242035909_-1480109735","id":"20191201-230408_483175924","dateCreated":"2019-12-01T23:13:55+0000","dateStarted":"2019-12-07T18:09:57+0000","dateFinished":"2019-12-07T18:10:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3277"},{"text":"\n","user":"anonymous","dateUpdated":"2019-12-04T21:35:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575242035909_1298224601","id":"20191201-230516_483125050","dateCreated":"2019-12-01T23:13:55+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3278"}],"name":"SparkCourse/code_03_XX_Data Ingestion with Spark and HDFS","id":"2EWT46BJS","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}